{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43f365a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary library\n",
    "import numpy as np\n",
    "from sklearn.decomposition import SparsePCA\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import ntpath\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a91d7d",
   "metadata": {},
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7383357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10000)\n",
      "(10000, 9)\n",
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "#Placing the input images into numpy array\n",
    "allData = []       \n",
    "happyData = []\n",
    "sadData = []\n",
    "N = 0\n",
    "nHappy = 0\n",
    "nSad = 0\n",
    "directory = r\".\\emotion_classification\\train\"\n",
    "for filename in os.scandir(directory):\n",
    "    if filename.is_file():\n",
    "        filename2 = directory + \"\\\\\" + ntpath.basename(filename)\n",
    "        img = Image.open(filename2).resize((100,100))\n",
    "        npImg = np.array(img)\n",
    "        flatArray = np.transpose(np.ravel(npImg))\n",
    "        x = ntpath.basename(filename).split(\".\")\n",
    "        if x[1] == \"happy\":\n",
    "            happyData.append(list(flatArray))\n",
    "            nHappy += 1\n",
    "        else:\n",
    "            sadData.append(list(flatArray))\n",
    "            nSad += 1\n",
    "N = nHappy + nSad\n",
    "happyData = np.transpose(np.array(happyData))\n",
    "sadData = np.transpose(np.array(sadData))\n",
    "allData = np.concatenate((happyData,sadData),axis = 1).T\n",
    "\n",
    "print(allData.shape)\n",
    "print(happyData.shape)\n",
    "print(sadData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172fc9e",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2d94055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "(10000, 6)\n",
      "(10000, 4)\n"
     ]
    }
   ],
   "source": [
    "#Placing the test images into numpy array\n",
    "happyDataTest = []\n",
    "sadDataTest = []\n",
    "N_test = 0\n",
    "nHappyTest = 0\n",
    "nSadTest = 0\n",
    "directory = r\".\\emotion_classification\\test\"\n",
    "for filename in os.scandir(directory):\n",
    "    if filename.is_file():\n",
    "        filename2 = directory + \"\\\\\" + ntpath.basename(filename)\n",
    "        img = Image.open(filename2).resize((100,100))\n",
    "        np_img = np.array(img)/1\n",
    "        flat_array = np.transpose(np.ravel(np_img))\n",
    "        x = ntpath.basename(filename).split(\".\")\n",
    "        if x[1] == \"happy\":\n",
    "            happyDataTest.append(list(flat_array))\n",
    "            nHappyTest += 1\n",
    "        else:\n",
    "            sadDataTest.append(list(flat_array))\n",
    "            nSadTest += 1 \n",
    "N_test = nHappyTest + nSadTest\n",
    "happyDataTest = np.transpose(np.array(happyDataTest))\n",
    "sadDataTest = np.transpose(np.array(sadDataTest))\n",
    "allDataTest = np.concatenate((happyDataTest,sadDataTest),axis = 1).T\n",
    "\n",
    "print(allDataTest.shape)\n",
    "print(happyDataTest.shape)\n",
    "print(sadDataTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2908dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "y_happy = [1 for i in range(nHappy)]\n",
    "y_sad = [0 for i in range(nSad)]\n",
    "y = y_happy + y_sad\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2311f8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "y_happyTest = [1 for i in range(nHappyTest)]\n",
    "y_sadTest = [0 for i in range(nSadTest)]\n",
    "y_test = y_happyTest + y_sadTest\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a05ebacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('DataQ5.txt','r+')\n",
    "reviews = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7740f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsList = reviews.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcd53e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyReviewsList = []\n",
    "positiveNegativeList = []\n",
    "for i in reviewsList:\n",
    "    onlyReviewsList.append(i[0:len(i)-4].lower())\n",
    "    positiveNegativeList.append(i[len(i)-1:])\n",
    "\n",
    "N = len(onlyReviewsList)\n",
    "trainSize = int(3/4 * N)\n",
    "testSize = int(1/4 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02cae154",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyReviewsList = np.array(onlyReviewsList)\n",
    "positiveNegativeList = np.array(positiveNegativeList)\n",
    "Tn = positiveNegativeList.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3772f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "TF_IDF = vectorizer.fit_transform(onlyReviewsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab2b80",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39161757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 3109)\n",
      "(250, 3109)\n"
     ]
    }
   ],
   "source": [
    "trainTF_IDF = TF_IDF[:trainSize,]\n",
    "testTF_IDF = TF_IDF[trainSize:,]\n",
    "print(trainTF_IDF.shape)\n",
    "print(testTF_IDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc129b0",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bbbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the dimension to 30 using PCA    TRAIN DATA\n",
    "temp = PCA(n_components = 30)                               #Error Need a dense matrix\n",
    "reducedTrainTF_IDF = temp.fit_transform(allData)              \n",
    "print(reducedTrainTF_IDF.shape)\n",
    "\n",
    "# Reducing the dimension to 30 using PCA   TEST DATA\n",
    "temp = PCA(n_components = 30)                               #Error Need a dense matrix\n",
    "testReducedTF_IDF = temp.fit_transform(allDataTest)              \n",
    "print(testReducedTF_IDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into Train and Validation\n",
    "trainSize = int(5/6 * 3/4 *N)\n",
    "validSize = int(1/6 * 3/4 *N)\n",
    "\n",
    "trainReducedTF_IDF = reducedTrainTF_IDF[:trainSize,]\n",
    "validReducedTF_IDF = reducedTrainTF_IDF[trainSize:,]\n",
    "trainTn = Tn[:trainSize]\n",
    "validTn = Tn[trainSize:trainSize + validSize]\n",
    "testTn = Tn[trainSize + validSize:]\n",
    "\n",
    "trainReducedTF_IDF = trainReducedTF_IDF.T\n",
    "validReducedTF_IDF = validReducedTF_IDF.T\n",
    "testReducedTF_IDF = testReducedTF_IDF.T\n",
    "\n",
    "print(trainReducedTF_IDF.shape)\n",
    "print(testReducedTF_IDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b400b1c",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df73e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegession:\n",
    "    def __init__(self, nIter, batchSize, lr, regularizationCoefficient = 0):\n",
    "        self.nIter = nIter\n",
    "        self.batchSize = batchSize\n",
    "        self.lr = lr\n",
    "        self.regularizationCoefficient = regularizationCoefficient\n",
    "        \n",
    "        #Initialize W and b\n",
    "        self.W = np.random.rand(30,1)\n",
    "        self.b = np.random.rand(1,1)\n",
    "        \n",
    "        #Loss list\n",
    "        self.lossTrain = []\n",
    "        self.lossValid = []\n",
    "        \n",
    "    def fit(self,rcFlag=0):\n",
    "        for j in range(self.nIter):\n",
    "            i = 0\n",
    "            while(True):\n",
    "                flag = 0\n",
    "                batchStartIndex = i * self.batchSize\n",
    "                batchEndIndex = (i+1) * self.batchSize\n",
    "                if (batchEndIndex>=trainSize):\n",
    "                    batchEndIndex = trainSize\n",
    "                    flag = 1\n",
    "                #Forward pass\n",
    "                Y = np.matmul((self.W).T, trainReducedTF_IDF) + self.b\n",
    "                Y = np.exp((-1) * Y)\n",
    "                Y = np.reciprocal(1 + Y)\n",
    "\n",
    "                #Backward pass\n",
    "                temp1 = Y.reshape(1,625)-trainTn.reshape(1,625)                      #temp1 = yn-tn\n",
    "                temp1 = temp1[0][batchStartIndex:batchEndIndex]\n",
    "                temp1 = temp1.reshape(1,batchEndIndex-batchStartIndex)\n",
    "\n",
    "                temp2 = trainReducedTF_IDF.T[batchStartIndex:batchEndIndex,]         #temp2 = x[batchSize]\n",
    "\n",
    "                if(rcFlag == 0):\n",
    "                    self.W = self.W - self.lr * (np.matmul(temp1, temp2)).T\n",
    "                else:\n",
    "                    self.W = self.W - self.lr * ((np.matmul(temp1, temp2)).T + self.regularizationCoefficient * self.W)\n",
    "                self.b = self.b - self.lr * np.sum(temp1)\n",
    "                \n",
    "                i += 1 \n",
    "                if(flag==1):\n",
    "                    break\n",
    "            \n",
    "            #Calculating Loss on Train Data\n",
    "            temp3 = trainTn.reshape(1,trainSize)\n",
    "            temp4 = Y.reshape(trainSize,1)\n",
    "            temp5 = np.matmul(temp3,np.log(temp4))\n",
    "            \n",
    "            temp6 = np.full((1,trainSize),1) - temp3\n",
    "            temp7 = np.full((trainSize,1),1) - temp4\n",
    "            temp8 = np.matmul(temp6,np.log(temp7))\n",
    "\n",
    "            calcLoss = -1 * (temp5 + temp8)            \n",
    "            self.lossTrain.append(int(calcLoss))\n",
    "            \n",
    "            #Calculating Loss on Validation Data\n",
    "            #Forming Y\n",
    "            Y_valid = np.matmul((self.W).T, validReducedTF_IDF) + self.b\n",
    "            Y_valid = np.exp((-1) * Y_valid)\n",
    "            Y_valid = np.reciprocal(1 + Y_valid)\n",
    "            \n",
    "            temp3 = validTn.reshape(1,validSize)\n",
    "            temp4 = Y_valid.reshape(validSize,1)\n",
    "            temp5 = np.matmul(temp3,np.log(temp4))\n",
    "            \n",
    "            temp6 = np.full((1,validSize),1) - temp3\n",
    "            temp7 = np.full((validSize,1),1) - temp4\n",
    "            temp8 = np.matmul(temp6,np.log(temp7))\n",
    "\n",
    "            calcLoss = -1 * (temp5 + temp8)            \n",
    "            self.lossValid.append(int(calcLoss))            \n",
    "            \n",
    "    def predict(self):\n",
    "        Y = np.matmul((self.W).T, testReducedTF_IDF) + self.b\n",
    "        Y = np.exp((-1) * Y)\n",
    "        Y = np.reciprocal(1 + Y)\n",
    "        \n",
    "        correct = 0\n",
    "        total = Y.shape[1]\n",
    "        predicted = []\n",
    "        for i in range(total):\n",
    "            if (Y[0][i]<0.5):\n",
    "                predicted.append(1)\n",
    "            else:\n",
    "                predicted.append(0)\n",
    "    \n",
    "        for i in range(total):\n",
    "            if(predicted[i] == int(testTn[i])):\n",
    "                correct += 1\n",
    "        accuracy = correct/total * 100\n",
    "        print('\\n'+'\\033[1m' + \"Review Classification Accuracy on Test Data = \", \"{:.2f}\".format(accuracy), \" %\" + '\\033[0m' )\n",
    "        \n",
    "    def plotLossCurve(self,y,flag):\n",
    "        x = [(i+1) for i in range(self.nIter)]\n",
    "\n",
    "        plt.xlabel('#epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        s = \"\"\n",
    "        if (flag == 0):\n",
    "            s = 'Train Data'\n",
    "        else:\n",
    "            s = 'Valid Data'\n",
    "        plt.title('Loss Curve on ' + s)\n",
    "        plt.plot(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def showLossValues(self,flag):\n",
    "        x =[]\n",
    "        if (flag == 0):\n",
    "            x = self.lossTrain\n",
    "            print('\\033[1m' + \"Loss values at each epoch for Train Data\" + '\\033[0m')\n",
    "        else:\n",
    "            x = self.lossValid\n",
    "            print('\\033[1m' + \"Loss values at each epoch for Valid Data\" + '\\033[0m')\n",
    "        \n",
    "        for i in range(self.nIter):\n",
    "            print((i+1),\"th Iteration->LossValue = \", x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df11115",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batchSizeList = [32, 64, 128]\n",
    "LRList = [1e-3, 1e-2, 1e-1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for bs in batchSizeList:\n",
    "    for lr in LRList:\n",
    "        k += 1\n",
    "        print(\"Case \" + str(k) + \": BatchSize = \" + str(bs) +\" and Learning Rate = \" + str(lr))\n",
    "        tempObj = LogisticRegession(epochs,bs,lr)\n",
    "        tempObj.fit()\n",
    "        tempObj.plotLossCurve(tempObj.lossTrain,0)\n",
    "        tempObj.showLossValues(0)\n",
    "        tempObj.plotLossCurve(tempObj.lossValid,1)\n",
    "        tempObj.showLossValues(1)\n",
    "        tempObj.predict()\n",
    "        print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de300b7c",
   "metadata": {},
   "source": [
    "Review classification accuracy for the nine cases is given below each cell. According to my result, I'm getting best accuracy of 61.2% in the following setting:\n",
    "\n",
    "Case 3: BatchSize = 32 and Learning Rate = 0.1\n",
    "Case 6: BatchSize = 64 and Learning Rate = 0.1\n",
    "Case 8: BatchSize = 128 and Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8d403",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RCList = [1e-2, 1e-1, 1]\n",
    "k = 0\n",
    "for bs in batchSizeList:\n",
    "    for lr in LRList:\n",
    "        for rc in RCList:\n",
    "            k += 1\n",
    "            print(\"Case \" + str(k) + \": BatchSize = \" + str(bs) +\" and Learning Rate = \" + str(lr) + \" and Regularization Coefficient = \" + str(rc))\n",
    "            DtempObj = LogisticRegession(epochs,bs,lr,rc)\n",
    "            DtempObj.fit(1)\n",
    "            DtempObj.plotLossCurve(DtempObj.lossTrain,0)\n",
    "            DtempObj.plotLossCurve(DtempObj.lossValid,1)\n",
    "            DtempObj.predict()\n",
    "            print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ead16ea6",
   "metadata": {},
   "source": [
    "Overfitting Observed for cases:\n",
    "Case 1: BatchSize = 32 and Learning Rate = 0.001 and Regularization Coefficient = 0.01\n",
    "Case 2: BatchSize = 32 and Learning Rate = 0.001 and Regularization Coefficient = 0.1\n",
    "Case 3: BatchSize = 32 and Learning Rate = 0.001 and Regularization Coefficient = 1\n",
    "Case 10: BatchSize = 64 and Learning Rate = 0.001 and Regularization Coefficient = 0.01\n",
    "Case 11: BatchSize = 64 and Learning Rate = 0.001 and Regularization Coefficient = 0.1\n",
    "Case 12: BatchSize = 64 and Learning Rate = 0.001 and Regularization Coefficient = 1\n",
    "Case 19: BatchSize = 128 and Learning Rate = 0.001 and Regularization Coefficient = 0.01\n",
    "Case 20: BatchSize = 128 and Learning Rate = 0.001 and Regularization Coefficient = 0.1\n",
    "Case 21: BatchSize = 128 and Learning Rate = 0.001 and Regularization Coefficient = 1\n",
    "\n",
    "Underfittting Observed for cases:\n",
    "Case 27: BatchSize = 128 and Learning Rate = 0.1 and Regularization Coefficient = 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
